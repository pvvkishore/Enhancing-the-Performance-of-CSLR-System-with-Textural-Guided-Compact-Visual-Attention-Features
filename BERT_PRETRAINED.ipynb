{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ff505a5-3d64-4c02-97c9-eb39421e8440",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel, BertForMaskedLM, BertConfig\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AdamW\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ba34ced-61b5-4b9a-9f09-c96b304c9f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SentenceDataset(Dataset):\n",
    "    def __init__(self, sentences, tokenizer, max_length):\n",
    "        self.sentences = sentences\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sentence = self.sentences[idx]\n",
    "        encoded_input = self.tokenizer(\n",
    "            sentence,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return encoded_input['input_ids'].squeeze(0), encoded_input['attention_mask'].squeeze(0)\n",
    "\n",
    "# Load sentences from your text file\n",
    "with open(\"C:/Users/prath/OneDrive/Desktop/200Sentences.txt\", \"r\") as file:\n",
    "    sentences = [line.strip() for line in file.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4915ddd-8cb3-4304-84bc-d31ad122e02a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bfcdfe3f2164ff5af945095085772e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prath\\anaconda3\\envs\\myenv\\Lib\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\prath\\.cache\\huggingface\\hub\\models--bert-large-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d262814ad094c489d9bccce6968f220",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e1b34ff51e948eba09406164e141db5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0972e315c054bb6bdb7afe8b3774d25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dc5d967a39748368f77902b2b549dfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.34G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "C:\\Users\\prath\\anaconda3\\envs\\myenv\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 1024, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 1024)\n",
       "      (token_type_embeddings): Embedding(2, 1024)\n",
       "      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-3): 4 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (transform_act_fn): GELUActivation()\n",
       "        (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=1024, out_features=30522, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenizer and model initialization\n",
    "pretrained_model = 'bert-large-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(pretrained_model)\n",
    "\n",
    "# Load the pretrained model\n",
    "model = BertForMaskedLM.from_pretrained(pretrained_model)\n",
    "\n",
    "# Modify the model to use only 4 transformer layers\n",
    "model.bert.encoder.layer = torch.nn.ModuleList(model.bert.encoder.layer[:4])\n",
    "model.config.num_hidden_layers = 4\n",
    "\n",
    "# Fine-tuning parameters\n",
    "max_length = 128  # Adjust based on your sentence lengths\n",
    "batch_size = 4    # Reduce batch size due to increased model size\n",
    "epochs = 5\n",
    "learning_rate = 5e-5\n",
    "\n",
    "# Dataset and DataLoader\n",
    "dataset = SentenceDataset(sentences, tokenizer, max_length)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Device configuration (use GPU if available)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69d91c77-58d2-4e59-9cff-b6a7721c114c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 0.6391, Accuracy: 93.61%\n",
      "Epoch 2/5, Loss: 0.0131, Accuracy: 99.83%\n",
      "Epoch 3/5, Loss: 0.0040, Accuracy: 99.97%\n",
      "Epoch 4/5, Loss: 0.0013, Accuracy: 100.00%\n",
      "Epoch 5/5, Loss: 0.0009, Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# Fine-tuning loop with accuracy\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for batch in dataloader:\n",
    "        input_ids, attention_mask = batch\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass and compute loss\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=input_ids)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Compute accuracy\n",
    "        predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "        correct = (predictions == input_ids).float().sum()\n",
    "        total_correct += correct.item()\n",
    "        total_samples += input_ids.numel()\n",
    "\n",
    "        # Backward pass and optimizer step\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = total_correct / total_samples * 100\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model.save_pretrained('./fine_tuned_bert_large')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66d763d9-b7bc-417d-b2d3-b7bcc796d6e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word embeddings saved in C:/Users/prath/OneDrive/Desktop/word_embeddings\n"
     ]
    }
   ],
   "source": [
    "# List of 40 target words for which you want to extract features\n",
    "target_words = ['hearingimpaired', 'communication', 'meetings', 'primeMinister', 'namaskar', 'indetail', 'watching', 'both', 'activities', 'chaired', 'children', 'development', 'earlier',  'fourteen',  'india', 'instructed', 'interaction', \n",
    "'inthis', 'more', 'movingon', 'one', 'reviewed', 'situation', 'technological', 'terrorists', 'thanks', 'thatsit', 'there', 'today', 'tools', 'under', 'yesterday', 'youare', 'health', 'imprisonment', 'phone', 'training', 'krishna', 'wrong', 'train']\n",
    "\n",
    "# Extracting Embedding Vectors for Multiple Words\n",
    "def get_embeddings_for_words(sentences, words, save_dir):\n",
    "    word_embeddings = {}\n",
    "    model.eval()\n",
    "\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        tokens = tokenizer(sentence, return_tensors='pt').to(device)\n",
    "        \n",
    "        # Get the embeddings from the fine-tuned BERT model\n",
    "        with torch.no_grad():\n",
    "            outputs = model.bert(**tokens)\n",
    "        \n",
    "        last_hidden_states = outputs.last_hidden_state  # Shape: (batch_size, sequence_length, hidden_size)\n",
    "\n",
    "        for word in words:\n",
    "            word_token_ids = tokenizer.encode(word, add_special_tokens=False)\n",
    "            word_indices = [i for i, token_id in enumerate(tokens['input_ids'][0]) if token_id in word_token_ids]\n",
    "\n",
    "            if word_indices:\n",
    "                # Extract the embeddings of the selected word\n",
    "                word_embeds = last_hidden_states[0, word_indices, :].mean(dim=0).cpu().numpy()\n",
    "\n",
    "                if word not in word_embeddings:\n",
    "                    word_embeddings[word] = []\n",
    "                word_embeddings[word].append(word_embeds)\n",
    "\n",
    "    # Average embeddings for each word across sentences and save to files\n",
    "    for word, embeds in word_embeddings.items():\n",
    "        averaged_word_embedding = np.mean(np.array(embeds), axis=0)\n",
    "        np.save(os.path.join(save_dir, f\"{word}.npy\"), averaged_word_embedding)\n",
    "\n",
    "    print(f\"Word embeddings saved in {save_dir}\")\n",
    "\n",
    "# Directory to save the extracted embeddings\n",
    "save_directory = \"C:/Users/prath/OneDrive/Desktop/word_embeddings\"\n",
    "\n",
    "# Extract features for the 40 target words and save them\n",
    "get_embeddings_for_words(sentences, target_words, save_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bba4961-bbbd-483c-8e81-fc608d68c3bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
