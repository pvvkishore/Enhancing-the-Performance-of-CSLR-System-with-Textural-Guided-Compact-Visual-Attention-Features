{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6302c848-bc9b-4660-ae2a-98ebec0aac04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# CrossModalAttention Module\n",
    "class CrossModalAttention(nn.Module):\n",
    "    def __init__(self, feature_dim):\n",
    "        super(CrossModalAttention, self).__init__()\n",
    "        self.query = nn.Linear(feature_dim, feature_dim)\n",
    "        self.key = nn.Linear(feature_dim, feature_dim)\n",
    "        self.value = nn.Linear(feature_dim, feature_dim)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "    \n",
    "    def forward(self, visual, textual):\n",
    "        \"\"\"\n",
    "        Forward pass of CrossModalAttention.\n",
    "        \"\"\"\n",
    "        # Ensure `visual` has three dimensions\n",
    "        if len(visual.shape) == 2:  # If visual is (B, Dim), add sequence dimension\n",
    "            visual = visual.unsqueeze(1)  # Shape becomes (B, 1, Dim)\n",
    "\n",
    "        # Ensure `textual` has three dimensions\n",
    "        if len(textual.shape) == 2:  # If textual is (B, Dim), add sequence dimension\n",
    "            textual = textual.unsqueeze(1)  # Shape becomes (B, 1, Dim)\n",
    "\n",
    "        # Linear transformations\n",
    "        query = self.query(textual)  # (B, 1, Dim)\n",
    "        key = self.key(visual)       # (B, Seq, Dim)\n",
    "        value = self.value(visual)   # (B, Seq, Dim)\n",
    "\n",
    "        # Transpose key for batch matrix multiplication\n",
    "        key = key.transpose(1, 2)  # (B, Dim, Seq)\n",
    "\n",
    "        # Attention weights\n",
    "        attention_weights = self.softmax(torch.bmm(query, key))  # (B, 1, Seq)\n",
    "\n",
    "        # Weighted sum of values\n",
    "        attended_features = torch.bmm(attention_weights, value)  # (B, 1, Dim)\n",
    "        attended_features = attended_features.squeeze(1)         # Remove sequence dimension\n",
    "\n",
    "        return attended_features, attention_weights\n",
    "\n",
    "\n",
    "# CrossModalModel with CrossModalAttention\n",
    "class CrossModalModel(nn.Module):\n",
    "    def __init__(self, feature_dim):\n",
    "        super(CrossModalModel, self).__init__()\n",
    "        self.cross_modal_attention = CrossModalAttention(feature_dim)\n",
    "        self.fc = nn.Linear(feature_dim, feature_dim)  # Optional final processing layer\n",
    "\n",
    "    def forward(self, visual, textual):\n",
    "        attended_features, attention_weights = self.cross_modal_attention(visual, textual)\n",
    "        visual_features = self.fc(attended_features)  # Optional processing\n",
    "        return visual_features, attention_weights\n",
    "\n",
    "\n",
    "# Dataset for Cross-Modal Data\n",
    "class CrossModalDataset(Dataset):\n",
    "    def __init__(self, bilstm_folder, embedding_folder):\n",
    "        self.bilstm_files = sorted(os.listdir(bilstm_folder))\n",
    "        self.embedding_files = sorted(os.listdir(embedding_folder))\n",
    "        self.bilstm_folder = bilstm_folder\n",
    "        self.embedding_folder = embedding_folder\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.bilstm_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        bilstm_path = os.path.join(self.bilstm_folder, self.bilstm_files[idx])\n",
    "        embedding_path = os.path.join(self.embedding_folder, self.embedding_files[idx])\n",
    "\n",
    "        visual_features = np.load(bilstm_path)  # Shape (Seq, Dim)\n",
    "        embedding_vector = np.load(embedding_path)  # Shape (1, Dim)\n",
    "\n",
    "        # Convert to tensors\n",
    "        visual_features = torch.tensor(visual_features, dtype=torch.float32)\n",
    "        embedding_vector = torch.tensor(embedding_vector, dtype=torch.float32)\n",
    "\n",
    "        return visual_features, embedding_vector\n",
    "\n",
    "\n",
    "# Loss Function for Cross-Modal Training\n",
    "def cross_modal_loss(predictions, targets):\n",
    "    return nn.MSELoss()(predictions, targets)\n",
    "\n",
    "\n",
    "# Training Function\n",
    "def train_model(dataset, feature_dim, epochs=10, lr=1e-4, batch_size=4):\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    model = CrossModalModel(feature_dim).cuda()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for visual, textual in dataloader:\n",
    "            visual, textual = visual.cuda(), textual.cuda()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            visual_features, _ = model(visual, textual)\n",
    "            loss = cross_modal_loss(visual_features, textual.squeeze(1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss / len(dataloader):.4f}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07c8b720-693f-4ef3-902e-cbf2d55c311a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.0339\n",
      "Epoch 2/10, Loss: 0.0147\n",
      "Epoch 3/10, Loss: 0.0100\n",
      "Epoch 4/10, Loss: 0.0075\n",
      "Epoch 5/10, Loss: 0.0060\n",
      "Epoch 6/10, Loss: 0.0047\n",
      "Epoch 7/10, Loss: 0.0039\n",
      "Epoch 8/10, Loss: 0.0032\n",
      "Epoch 9/10, Loss: 0.0027\n",
      "Epoch 10/10, Loss: 0.0023\n"
     ]
    }
   ],
   "source": [
    "# Main Code\n",
    "if __name__ == \"__main__\":\n",
    "    bilstm_folder = \"C:/Users/prath/OneDrive/Desktop/bilstm_features_10\" # Path to BiLSTM features folder\n",
    "    embedding_folder = \"C:/Users/prath/OneDrive/Desktop/paper2/embedding_vectors_1_10\"  # Folder containing embedding .npy files\n",
    "\n",
    "    feature_dim =  1024  # Dimension of the features\n",
    "\n",
    "    dataset = CrossModalDataset(bilstm_folder, embedding_folder)\n",
    "    trained_model = train_model(dataset, feature_dim)\n",
    "\n",
    "    # Save the trained model\n",
    "    torch.save(trained_model.state_dict(), \"cross_modal_model.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29e8189e-24df-4bf0-ab1a-2f0b1ac89aab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prath\\AppData\\Local\\Temp\\ipykernel_14524\\4022805966.py:42: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"cross_modal_model.pth\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved attended feature to C:/Users/prath/OneDrive/Desktop/attended_features10\\1.npy\n",
      "Saved attended feature to C:/Users/prath/OneDrive/Desktop/attended_features10\\2.npy\n",
      "Saved attended feature to C:/Users/prath/OneDrive/Desktop/attended_features10\\3.npy\n",
      "Saved attended feature to C:/Users/prath/OneDrive/Desktop/attended_features10\\4.npy\n",
      "Saved attended feature to C:/Users/prath/OneDrive/Desktop/attended_features10\\5.npy\n",
      "Saved attended feature to C:/Users/prath/OneDrive/Desktop/attended_features10\\6.npy\n",
      "Saved attended feature to C:/Users/prath/OneDrive/Desktop/attended_features10\\7.npy\n",
      "Saved attended feature to C:/Users/prath/OneDrive/Desktop/attended_features10\\8.npy\n",
      "Saved attended feature to C:/Users/prath/OneDrive/Desktop/attended_features10\\9.npy\n",
      "Saved attended feature to C:/Users/prath/OneDrive/Desktop/attended_features10\\10.npy\n",
      "Saved attended feature to C:/Users/prath/OneDrive/Desktop/attended_features10\\11.npy\n",
      "Saved attended feature to C:/Users/prath/OneDrive/Desktop/attended_features10\\12.npy\n",
      "Saved attended feature to C:/Users/prath/OneDrive/Desktop/attended_features10\\13.npy\n",
      "Saved attended feature to C:/Users/prath/OneDrive/Desktop/attended_features10\\14.npy\n",
      "Saved attended feature to C:/Users/prath/OneDrive/Desktop/attended_features10\\15.npy\n",
      "Saved attended feature to C:/Users/prath/OneDrive/Desktop/attended_features10\\16.npy\n",
      "Saved attended feature to C:/Users/prath/OneDrive/Desktop/attended_features10\\17.npy\n",
      "Saved attended feature to C:/Users/prath/OneDrive/Desktop/attended_features10\\18.npy\n",
      "Saved attended feature to C:/Users/prath/OneDrive/Desktop/attended_features10\\19.npy\n",
      "Saved attended feature to C:/Users/prath/OneDrive/Desktop/attended_features10\\20.npy\n",
      "Saved attended feature to C:/Users/prath/OneDrive/Desktop/attended_features10\\21.npy\n",
      "Saved attended feature to C:/Users/prath/OneDrive/Desktop/attended_features10\\22.npy\n",
      "Saved attended feature to C:/Users/prath/OneDrive/Desktop/attended_features10\\23.npy\n",
      "Saved attended feature to C:/Users/prath/OneDrive/Desktop/attended_features10\\24.npy\n",
      "Saved attended feature to C:/Users/prath/OneDrive/Desktop/attended_features10\\25.npy\n",
      "Saved attended feature to C:/Users/prath/OneDrive/Desktop/attended_features10\\26.npy\n",
      "Saved attended feature to C:/Users/prath/OneDrive/Desktop/attended_features10\\27.npy\n",
      "Saved attended feature to C:/Users/prath/OneDrive/Desktop/attended_features10\\28.npy\n",
      "Saved attended feature to C:/Users/prath/OneDrive/Desktop/attended_features10\\29.npy\n",
      "Saved attended feature to C:/Users/prath/OneDrive/Desktop/attended_features10\\30.npy\n",
      "Saved attended feature to C:/Users/prath/OneDrive/Desktop/attended_features10\\31.npy\n",
      "Saved attended feature to C:/Users/prath/OneDrive/Desktop/attended_features10\\32.npy\n",
      "Saved attended feature to C:/Users/prath/OneDrive/Desktop/attended_features10\\33.npy\n",
      "Saved attended feature to C:/Users/prath/OneDrive/Desktop/attended_features10\\34.npy\n",
      "Saved attended feature to C:/Users/prath/OneDrive/Desktop/attended_features10\\35.npy\n",
      "Saved attended feature to C:/Users/prath/OneDrive/Desktop/attended_features10\\36.npy\n",
      "Saved attended feature to C:/Users/prath/OneDrive/Desktop/attended_features10\\37.npy\n",
      "Saved attended feature to C:/Users/prath/OneDrive/Desktop/attended_features10\\38.npy\n",
      "Saved attended feature to C:/Users/prath/OneDrive/Desktop/attended_features10\\39.npy\n",
      "Saved attended feature to C:/Users/prath/OneDrive/Desktop/attended_features10\\40.npy\n",
      "Saved attended feature to C:/Users/prath/OneDrive/Desktop/attended_features10\\41.npy\n",
      "Saved attended feature to C:/Users/prath/OneDrive/Desktop/attended_features10\\42.npy\n",
      "Saved attended feature to C:/Users/prath/OneDrive/Desktop/attended_features10\\43.npy\n",
      "Saved attended feature to C:/Users/prath/OneDrive/Desktop/attended_features10\\44.npy\n",
      "Saved attended feature to C:/Users/prath/OneDrive/Desktop/attended_features10\\45.npy\n",
      "Saved attended feature to C:/Users/prath/OneDrive/Desktop/attended_features10\\46.npy\n",
      "Saved attended feature to C:/Users/prath/OneDrive/Desktop/attended_features10\\47.npy\n",
      "Saved attended feature to C:/Users/prath/OneDrive/Desktop/attended_features10\\48.npy\n",
      "Saved attended feature to C:/Users/prath/OneDrive/Desktop/attended_features10\\49.npy\n",
      "Saved attended feature to C:/Users/prath/OneDrive/Desktop/attended_features10\\50.npy\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def extract_attended_features(model, dataset, output_folder, batch_size=4):\n",
    "    \"\"\"\n",
    "    Extract attended visual features for the dataset and save them.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained CrossModalModel.\n",
    "        dataset: Dataset containing visual and textual inputs.\n",
    "        output_folder: Folder to save the attended features.\n",
    "        batch_size: Batch size for processing.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx, (visual, textual) in enumerate(dataloader):\n",
    "            visual, textual = visual.cuda(), textual.cuda()\n",
    "            attended_features, _ = model(visual, textual)  # Get attended features\n",
    "            \n",
    "            # Save each batch of features\n",
    "            for i in range(attended_features.size(0)):\n",
    "                feature_path = os.path.join(output_folder, f\"{idx * batch_size + i + 1}.npy\")\n",
    "                np.save(feature_path, attended_features[i].cpu().numpy())\n",
    "                print(f\"Saved attended feature to {feature_path}\")\n",
    "\n",
    "# Usage\n",
    "if __name__ == \"__main__\":\n",
    "    bilstm_folder = \"C:/Users/prath/OneDrive/Desktop/bilstm_features_10\"\n",
    "    embedding_folder = \"C:/Users/prath/OneDrive/Desktop/paper2/embedding_vectors_1_10\"\n",
    "    output_folder = \"C:/Users/prath/OneDrive/Desktop/attended_features10\"\n",
    "\n",
    "    feature_dim = 1024\n",
    "    dataset = CrossModalDataset(bilstm_folder, embedding_folder)\n",
    "\n",
    "    # Load trained model\n",
    "    model = CrossModalModel(feature_dim).cuda()\n",
    "    model.load_state_dict(torch.load(\"cross_modal_model.pth\"))\n",
    "\n",
    "    # Extract and save features\n",
    "    extract_attended_features(model, dataset, output_folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389ee333-22fb-4533-9e7d-6952c766b9d8",
   "metadata": {},
   "source": [
    "COMPACT BI-LINEAR POOLING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8cd94b30-244e-4404-be12-da8a14208d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved pooled features for 1.npy to C:/Users/prath/OneDrive/Desktop/pooled features10\\1.npy\n",
      "Processed and saved pooled features for 10.npy to C:/Users/prath/OneDrive/Desktop/pooled features10\\10.npy\n",
      "Processed and saved pooled features for 11.npy to C:/Users/prath/OneDrive/Desktop/pooled features10\\11.npy\n",
      "Processed and saved pooled features for 12.npy to C:/Users/prath/OneDrive/Desktop/pooled features10\\12.npy\n",
      "Processed and saved pooled features for 13.npy to C:/Users/prath/OneDrive/Desktop/pooled features10\\13.npy\n",
      "Processed and saved pooled features for 14.npy to C:/Users/prath/OneDrive/Desktop/pooled features10\\14.npy\n",
      "Processed and saved pooled features for 15.npy to C:/Users/prath/OneDrive/Desktop/pooled features10\\15.npy\n",
      "Processed and saved pooled features for 16.npy to C:/Users/prath/OneDrive/Desktop/pooled features10\\16.npy\n",
      "Processed and saved pooled features for 17.npy to C:/Users/prath/OneDrive/Desktop/pooled features10\\17.npy\n",
      "Processed and saved pooled features for 18.npy to C:/Users/prath/OneDrive/Desktop/pooled features10\\18.npy\n",
      "Processed and saved pooled features for 19.npy to C:/Users/prath/OneDrive/Desktop/pooled features10\\19.npy\n",
      "Processed and saved pooled features for 2.npy to C:/Users/prath/OneDrive/Desktop/pooled features10\\2.npy\n",
      "Processed and saved pooled features for 20.npy to C:/Users/prath/OneDrive/Desktop/pooled features10\\20.npy\n",
      "Processed and saved pooled features for 21.npy to C:/Users/prath/OneDrive/Desktop/pooled features10\\21.npy\n",
      "Processed and saved pooled features for 22.npy to C:/Users/prath/OneDrive/Desktop/pooled features10\\22.npy\n",
      "Processed and saved pooled features for 23.npy to C:/Users/prath/OneDrive/Desktop/pooled features10\\23.npy\n",
      "Processed and saved pooled features for 24.npy to C:/Users/prath/OneDrive/Desktop/pooled features10\\24.npy\n",
      "Processed and saved pooled features for 25.npy to C:/Users/prath/OneDrive/Desktop/pooled features10\\25.npy\n",
      "Processed and saved pooled features for 26.npy to C:/Users/prath/OneDrive/Desktop/pooled features10\\26.npy\n",
      "Processed and saved pooled features for 27.npy to C:/Users/prath/OneDrive/Desktop/pooled features10\\27.npy\n",
      "Processed and saved pooled features for 28.npy to C:/Users/prath/OneDrive/Desktop/pooled features10\\28.npy\n",
      "Processed and saved pooled features for 29.npy to C:/Users/prath/OneDrive/Desktop/pooled features10\\29.npy\n",
      "Processed and saved pooled features for 3.npy to C:/Users/prath/OneDrive/Desktop/pooled features10\\3.npy\n",
      "Processed and saved pooled features for 30.npy to C:/Users/prath/OneDrive/Desktop/pooled features10\\30.npy\n",
      "Processed and saved pooled features for 31.npy to C:/Users/prath/OneDrive/Desktop/pooled features10\\31.npy\n",
      "Processed and saved pooled features for 32.npy to C:/Users/prath/OneDrive/Desktop/pooled features10\\32.npy\n",
      "Processed and saved pooled features for 33.npy to C:/Users/prath/OneDrive/Desktop/pooled features10\\33.npy\n",
      "Processed and saved pooled features for 34.npy to C:/Users/prath/OneDrive/Desktop/pooled features10\\34.npy\n",
      "Processed and saved pooled features for 35.npy to C:/Users/prath/OneDrive/Desktop/pooled features10\\35.npy\n",
      "Processed and saved pooled features for 36.npy to C:/Users/prath/OneDrive/Desktop/pooled features10\\36.npy\n",
      "Processed and saved pooled features for 37.npy to C:/Users/prath/OneDrive/Desktop/pooled features10\\37.npy\n",
      "Processed and saved pooled features for 38.npy to C:/Users/prath/OneDrive/Desktop/pooled features10\\38.npy\n",
      "Processed and saved pooled features for 39.npy to C:/Users/prath/OneDrive/Desktop/pooled features10\\39.npy\n",
      "Processed and saved pooled features for 4.npy to C:/Users/prath/OneDrive/Desktop/pooled features10\\4.npy\n",
      "Processed and saved pooled features for 40.npy to C:/Users/prath/OneDrive/Desktop/pooled features10\\40.npy\n",
      "Processed and saved pooled features for 41.npy to C:/Users/prath/OneDrive/Desktop/pooled features10\\41.npy\n",
      "Processed and saved pooled features for 42.npy to C:/Users/prath/OneDrive/Desktop/pooled features10\\42.npy\n",
      "Processed and saved pooled features for 43.npy to C:/Users/prath/OneDrive/Desktop/pooled features10\\43.npy\n",
      "Processed and saved pooled features for 44.npy to C:/Users/prath/OneDrive/Desktop/pooled features10\\44.npy\n",
      "Processed and saved pooled features for 45.npy to C:/Users/prath/OneDrive/Desktop/pooled features10\\45.npy\n",
      "Processed and saved pooled features for 46.npy to C:/Users/prath/OneDrive/Desktop/pooled features10\\46.npy\n",
      "Processed and saved pooled features for 47.npy to C:/Users/prath/OneDrive/Desktop/pooled features10\\47.npy\n",
      "Processed and saved pooled features for 48.npy to C:/Users/prath/OneDrive/Desktop/pooled features10\\48.npy\n",
      "Processed and saved pooled features for 49.npy to C:/Users/prath/OneDrive/Desktop/pooled features10\\49.npy\n",
      "Processed and saved pooled features for 5.npy to C:/Users/prath/OneDrive/Desktop/pooled features10\\5.npy\n",
      "Processed and saved pooled features for 50.npy to C:/Users/prath/OneDrive/Desktop/pooled features10\\50.npy\n",
      "Processed and saved pooled features for 6.npy to C:/Users/prath/OneDrive/Desktop/pooled features10\\6.npy\n",
      "Processed and saved pooled features for 7.npy to C:/Users/prath/OneDrive/Desktop/pooled features10\\7.npy\n",
      "Processed and saved pooled features for 8.npy to C:/Users/prath/OneDrive/Desktop/pooled features10\\8.npy\n",
      "Processed and saved pooled features for 9.npy to C:/Users/prath/OneDrive/Desktop/pooled features10\\9.npy\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.functional import normalize\n",
    "\n",
    "# Compact Bilinear Pooling Implementation\n",
    "class CompactBilinearPooling(nn.Module):\n",
    "    def __init__(self, input_dim1, input_dim2, output_dim):\n",
    "        super(CompactBilinearPooling, self).__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.sketch1 = nn.Parameter(torch.randint(0, output_dim, (input_dim1,), dtype=torch.long), requires_grad=False)\n",
    "        self.sign1 = nn.Parameter(torch.randint(0, 2, (input_dim1,), dtype=torch.float32) * 2 - 1, requires_grad=False)\n",
    "        self.sketch2 = nn.Parameter(torch.randint(0, output_dim, (input_dim2,), dtype=torch.long), requires_grad=False)\n",
    "        self.sign2 = nn.Parameter(torch.randint(0, 2, (input_dim2,), dtype=torch.float32) * 2 - 1, requires_grad=False)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        # Create empty tensors for the sketches\n",
    "        x1_sketch = torch.zeros(x1.size(0), self.output_dim, device=x1.device)\n",
    "        x2_sketch = torch.zeros(x2.size(0), self.output_dim, device=x2.device)\n",
    "\n",
    "        # Compute sketches for x1\n",
    "        for i in range(x1.size(1)):\n",
    "            idx = self.sketch1[i].item()\n",
    "            x1_sketch[:, idx] += x1[:, i] * self.sign1[i]\n",
    "\n",
    "        # Compute sketches for x2\n",
    "        for i in range(x2.size(1)):\n",
    "            idx = self.sketch2[i].item()\n",
    "            x2_sketch[:, idx] += x2[:, i] * self.sign2[i]\n",
    "\n",
    "        # Perform FFT, element-wise multiplication, and inverse FFT\n",
    "        fft_x1 = torch.fft.rfft(x1_sketch, dim=1)\n",
    "        fft_x2 = torch.fft.rfft(x2_sketch, dim=1)\n",
    "        fft_product = fft_x1 * fft_x2\n",
    "        result = torch.fft.irfft(fft_product, n=self.output_dim, dim=1)\n",
    "\n",
    "        return normalize(result, p=2, dim=1)  # Normalize the output\n",
    "\n",
    "    \n",
    "   # Paths to your data folders\n",
    "attended_features_folder = \"C:/Users/prath/OneDrive/Desktop/attended_features10\"\n",
    "embedding_vectors_folder = \"C:/Users/prath/OneDrive/Desktop/paper2/embedding_vectors_1_10\"\n",
    "output_folder = \"C:/Users/prath/OneDrive/Desktop/pooled features10\"\n",
    "\n",
    "# Hyperparameters\n",
    "input_dim1, input_dim2, output_dim = 1024, 1024, 2048  # Adjust dimensions as needed\n",
    "cbp = CompactBilinearPooling(input_dim1, input_dim2, output_dim)\n",
    "\n",
    "# Ensure output folder exists\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Process data\n",
    "for filename in os.listdir(attended_features_folder):\n",
    "    # Load attended visual features and corresponding embedding vector\n",
    "    attended_path = os.path.join(attended_features_folder, filename)\n",
    "    embedding_path = os.path.join(embedding_vectors_folder, filename)  # Assuming matching filenames\n",
    "\n",
    "    if os.path.isfile(attended_path) and os.path.isfile(embedding_path):\n",
    "        attended_visual_features = torch.tensor(np.load(attended_path), dtype=torch.float32)  # Shape: (batch_size, 1024)\n",
    "        embedding_vectors = torch.tensor(np.load(embedding_path), dtype=torch.float32)        # Shape: (batch_size, 1024)\n",
    "\n",
    "        # Reshape attended visual features to match (1, 1024)\n",
    "        attended_visual_features = attended_visual_features.unsqueeze(0)  # Add batch dimension: (1, 1024)\n",
    "        \n",
    "        # Ensure batch dimensions are aligned\n",
    "        if attended_visual_features.shape[1] != embedding_vectors.shape[1]:\n",
    "            print(f\"Feature size mismatch for {filename}. Skipping...\")\n",
    "            continue\n",
    "\n",
    "        # Perform Compact Bilinear Pooling\n",
    "        pooled_features = cbp(attended_visual_features, embedding_vectors)\n",
    "\n",
    "        # Save pooled features\n",
    "        output_path = os.path.join(output_folder, filename)\n",
    "        np.save(output_path, pooled_features.detach().cpu().numpy())\n",
    "        print(f\"Processed and saved pooled features for {filename} to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb3627a-4564-4f03-85ec-09158d4770bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
